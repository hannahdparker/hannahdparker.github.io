---
title: 'Getting Data'
date: 3/23/2017
output: html_document
---

You're interested in doing some data analysis, that's clear. Unfortunately you're missing something important...... Data! There's plenty of sources of NBA data, and luckily for us, most of it's in a very structured format. 

In this post, I'm going to show you how to get different types of data from a couple of different sources. The focus is going to be on scraping your own data from websites. You may be used to copying and pasting data into a spreadsheet or downloading it in csv form. While this method is straightforward, when trying to collect large amounts of data this method can be very cumbersome. 

#Scraping a Basic HTML Table

The first example we're going to go over is scraping standings from USA Today. The standings are in a basic HTML format. We'll be using the dplyr and rvest libraries throughout this post, so let's load them up now.

```{r, message=F, warning=F}
library(dplyr)
library(rvest)
```

Now we'll store the url we are going to be scraping from in an object called "usa".

```{r}
usa<- "http://www.usatoday.com/sports/nba/standings/"
```

Now it's time to scrape some data. What you'll need is the CSS selector. The method for getting this can be different based on what browser you use. I use firefox, and I simply right click on the table and click on inspect element. I then look for the line of code in the inspector that covers the table (the table will be highlighted once you hover over the code). I then copy the selector. In this example it's #DataTables_Table_0.

Using dplyr, we scrape the data and store it in u.standings. We paste the css selector in the command read_html(). Because the data we are scraping is in table format, we use the command html_table(), which parses the table into a data frame.

```{r}
u.standings<-
  usa %>%
  read_html('#DataTables_Table_0') %>%
  html_table()
```

The data is stored in a list, split by the separate conferences. Let's change the list into two data frames, one for the east and one for the west.

```{r}
east<- data.frame(u.standings[1])
west<- data.frame(u.standings[2])

head(east, 3)
```

Unfortunately there is a pretty big issue with the data; any column with a hyphen is treated as a character column. To deal with this in the Games Back column (GB), we'll simply replace the hyphen with a zero, since they mean the same thing (a team in first place is zero games back of first place).

```{r}
east[east=="-"]<- "0"
west[west=="-"]<- "0"
```

Dealing with the home, road, conference and last 10 game records is a little more difficult. Let's split up these records into separate win and loss columns (i.e. home wins and home losses). The following dplyr code shows how to do this for the east dataframe (the same can be done with the west by replacing the dataframe in the code). 

The win and loss columns are created using the substring command and then each of the newly created columns are made numeric. Substring is simply grabbing the character based on the index argument. So, taking Home_Wins for example, we're grabbing the characters starting at the first element and ending at the second element. The dataframe is now ready for analysis!

```{r, echo=1}
east<-
  east %>%
  mutate(Home_Wins = substring(HOME, 1, 2), Away_Wins = substring(ROAD, 1, 2), 
         Conf_Wins = substring(CONF, 1, 2), L.10_Wins = substring(L.10, 1, 1), 
         Home_Loss = substring(HOME, 5, 7), Away_Loss = substring(ROAD, 5, 7), 
         Conf_Loss = substring(CONF, 5, 7), L.10_Loss = substring(L.10, 3, 3)) %>%
  mutate_each(funs(as.numeric), c(GB, Home_Wins:L.10_Loss)) %>%
  select(-c(HOME:CONF, L.10))

west<-
  west %>%
  mutate(Home_Wins = substring(HOME, 1, 2), Away_Wins = substring(ROAD, 1, 2), 
         Conf_Wins = substring(CONF, 1, 2), L.10_Wins = substring(L.10, 1, 1), 
         Home_Loss = substring(HOME, 5, 7), Away_Loss = substring(ROAD, 5, 7), 
         Conf_Loss = substring(CONF, 5, 7), L.10_Loss = substring(L.10, 3, 3)) %>%
  mutate_each(funs(as.numeric), c(GB, Home_Wins:L.10_Loss)) %>%
  select(-c(HOME:CONF, L.10))

```


#HTML Scraping Continued

Next, I want to go over an example that's a bit more difficult. The focus on this section is more about dealing with a certain website rather than the scraping itself, but it's going to be a website you'll probably use a lot if you want to analyze NBA stats: Basketball Reference. 

Basketball Reference makes their data easy to download, so if you just want one specific table it may be easier to just go to the website and download a csv from the website. However, we're going to go over an example which scrapes multiple tables.

The tables on Basketball Reference are in html format and some are easy to scrape; however, some tables are hidden inside comments, which make them difficult to deal with. We're going to work with a case where we have to get these hidden tables.

In this example we're going to scrape the Indiana Pacers' advanced stats for the past three seasons. An example of one of the tables we're gonna scrape is found here: http://www.basketball-reference.com/teams/IND/2017.html. We're going to start by creating a list of urls for each of the three seasons. We're going to use sapply to paste each respective season into the Indiana Pacers' team url and store it in an object called urls.

```{r}
urls<- sapply(as.character(2015:2017), 
              function(x) 
                paste("http://www.basketball-reference.com/teams/IND/", x, ".html", sep=""))
```

Now we're going to scrape the tables. We're going to apply a scraping function to each of the urls and store them in a list called adv.pacers. We use the same method explained in the last section of identifying the css selector, in this case it's #advanced. Before we scrape the data, we have to parse the comments, but once the comments are parsed, the tables are easily scraped. 

```{r}
adv.pacers<-
  lapply(urls, 
         function(x)
           read_html(x) %>%
           #code to parse the comments
           html_nodes(xpath='//comment()') %>%
           html_text() %>%
           paste(collapse='') %>%
           #now go about the usual scraping
           read_html() %>%
           html_node('#advanced') %>%
           html_table())
  
```

Looking at the first three rows of each data frame in the list, we can see we were successful!

```{r}
lapply(adv.pacers, function(x) head(x, 3))
```

#Scraping JSON Data

Another great source for NBA data is the NBA's own stats website. Getting data from the NBA site is a little different as it's stored in a JSON format rather than HTML. Don't let this scare you off, though, as getting data is just as easy!

There are multiple R libraries for scraping JSON data, but the one I'm most familiar with is jsonlite.

```{r, warning=F}
library(jsonlite)
```

We're going to scrape the traditional stats table for Kyle Korver found at this link: http://stats.nba.com/player/#!/201935/. I am using firefox again in this instance, but chrome has a similar method of doing this. 

First you need to jump into the Developer tool bar in firefox and click toggle tools. Next you're going to click on the Network Monitor header followed by the XHR tab under the Network Monitor header. Now refresh the web page and you should be able to see several requests in the toolbar. You need to search around for the url we want, but you do have previews available (look at the previews by clicking on "response" in the tool bar with the options: headers, cookies, params, and response). After looking through a few of the requests, I found a url that has a parameter labeled PerGame. Since we want traditional per game stats, this sounds right.

Let's try bringing the data into R. First, we'll use the fromJSON command on the url we got from the NBA stats website. the readLines command reads in the url and returns the source data from the web page.

```{r, warning=F}
korver<- fromJSON(readLines("http://stats.nba.com/stats/playerdashboardbyyearoveryear?DateFrom=&DateTo=&GameSegment=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerID=2594&PlusMinus=N&Rank=N&Season=2016-17&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&Split=yoy&VsConference=&VsDivision="))
```

The data is now in a list called korver. To get the data we want we need to check out the result sets. The row set has all of the actual data we want. Pulling that up gives us this (note, I'm only pulling up the first three rows of columns 11 to 15, just to make things less messy): 

```{r}
korver.df<- data.frame(korver$resultSets$rowSet)
head(korver.df[,11:15], 3)
```

That's a lot of information, and it doesn't have column names... It looks really confusing. Luckily, the column names are in the data we scraped, just under the headers section in resultSets. There are a list of two headers, both of which are the same; this is because the scraped dataframe has the same columns listed twice. Let's officially assign these to be the column names of korver.df.

```{r}
korver$resultSets$headers[[1]]

colnames(korver.df)<- korver$resultSets$headers[[1]]
```

Now our data looks a little more understandable... The scraped data includes a lot of columns that aren't shown on the original website table, such as MAX_GAME_DATE, Wins, and the rank of each season's stats on Korver's career. You guys can easily get rid of whatever you deem unimportant, and if you end up changing your mind on stuff you got rid of, you can just scrape it again! Also be aware, the scraped data is read in as a factor, so you need to change up stuff that you don't want to be considered a factor.

```{r}
head(korver.df[,11:15], 3)
```

These few examples should be a good way to get your guys' hands on some data. Although these examples are related to a few NBA specific websites, they can easily be applied to websites with other information. 

#References and Other Tutorials

* http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/: Greg Reda does a great job of explaining scraping json data in depth. Even better, he uses another stats.nba example!

* https://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/: An intro to the rvest package.

* http://asbcllc.com/blog/2014/november/creating_bref_scraper/: Scraping example using basketball reference from Alex Bresler

* http://stats.nba.com/

* http://www.basketball-reference.com/

* http://www.usatoday.com/